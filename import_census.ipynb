{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from random import sample\n",
    "\n",
    "sys.path.append('/opt/cloudera/parcels/SPARK2/lib/spark2/python')\n",
    "sys.path.append('/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip')\n",
    "\n",
    "from itertools import chain, zip_longest\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from subprocess import check_output, Popen, PIPE, check_call\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from spark_utils.spark_env import (read_spark_env_json, read_spark_conf_json, \n",
    "                       set_spark_env, create_spark_conf)\n",
    "from spark_utils.clean_dataworld import process_file, process_record\n",
    "\n",
    "spark_env = read_spark_env_json()\n",
    "spark_conf = read_spark_conf_json()\n",
    "conf = create_spark_conf()\n",
    "conf.set('spark.executor.memory', str(7 * 1024 - 881) + 'm')\n",
    "conf.set('spark.dynamicAllocation.minExecutors', '18')\n",
    "conf.set('spark.dynamicAllocation.initialExecutors', '18')\n",
    "conf.set('spark.executor.instances', '18')\n",
    "conf.set('spark.yarn.executor.memoryOverhead', str(int((7 * 1024 - 881) * 0.2)) + 'm')\n",
    "conf.set('spark.executor.cores', '2')\n",
    "\n",
    "cmd = ['zip', '-r', 'spark_utils.zip', 'spark_utils', '-i']\n",
    "cmd.extend(['spark_utils/' + f for f in os.listdir('spark_utils') if f.endswith('.py')])\n",
    "check_call(cmd)\n",
    "\n",
    "set_spark_env(spark_env)\n",
    "\n",
    "sc = SparkContext(conf=conf, pyFiles=['spark_utils.zip'])\n",
    "sql = SQLContext(sc)\n",
    "\n",
    "with open('spark-conf.json', 'w') as f:\n",
    "    f.write(json.dumps(dict(conf.getAll()), indent=4))\n",
    "# keys = list(dict(conf.getAll()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1257m'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.get('spark.yarn.executor.memoryOverhead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ontology = '\\n'.join(sc.textFile('/user/hdfs/dataworld-linked-acs/Ontology/acs_schema.ttl').collect())\n",
    "ontology = re.split('\\n\\.\\n', ontology)\n",
    "# fnames = check_output(['hdfs', 'dfs', '-ls', '/user/hdfs/dataworld/EnhancedData/ss*']).decode().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = check_output(['hdfs', 'dfs', '-ls', '/user/hdfs/dataworld-linked-acs']).decode().split('\\n')\n",
    "reg = re.compile('.*ss[0-9]{2}p.*\\.ttl\\.gz$')\n",
    "person_files = [f2.split()[-1] for f2 in [f.strip() for f in fnames] if reg.search(f2)]\n",
    "hive_fnames = check_output(['hdfs', 'dfs', '-ls', '/user/hive/warehouse/dataworld/person/**/**']).decode().split('\\n')\n",
    "hive_reg = re.compile('.*/state=(?P<state>[a-z]{2})/year=(?P<year>[0-9]{2})/.*parquet$')\n",
    "state_reg = re.compile('.*ss(?P<year>[0-9]{2})(h|p)(?P<state>[a-z]{2})(\\.[0-9])?\\.ttl\\.gz$')\n",
    "state_year_hive = [(h['state'], h['year']) for h in [hive_reg.search(f) for f in hive_fnames] if h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_closure = partial(process_file, kind='person')\n",
    "state_reg = re.compile('.*ss(?P<year>[0-9]{2})(h|p)(?P<state>[a-z]{2})(\\.[0-9])?\\.ttl\\.gz$')\n",
    "# f = [p for p in person_files if p.endswith('15pfl.1.ttl.gz')]\n",
    "i = 0\n",
    "n_executors = int(sc.getConf().get('spark.dynamicAllocation.initialExecutors'))\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping group starting with file /user/hdfs/dataworld-linked-acs/ss14pak.ttl.gz\n",
      "skipping group starting with file /user/hdfs/dataworld-linked-acs/ss14pil.1.ttl.gz\n",
      "skipping group starting with file /user/hdfs/dataworld-linked-acs/ss14pnj.ttl.gz\n",
      "skipping group starting with file /user/hdfs/dataworld-linked-acs/ss14ptx.3.ttl.gz\n",
      "skipping group starting with file /user/hdfs/dataworld-linked-acs/ss15pdc.ttl.gz\n",
      "skipping group starting with file /user/hdfs/dataworld-linked-acs/ss15pmn.ttl.gz\n"
     ]
    }
   ],
   "source": [
    "def get_keys(x):\n",
    "    '''Take a sample from each file and get the questions of the survey.\n",
    "    '''\n",
    "    return list(set(chain(*[list(d.keys()) for d in x])))\n",
    "\n",
    "for group in grouper(person_files, n_executors):\n",
    "    state_years = [(r['state'], r['year']) not in state_year_hive for r in [state_reg.search(f) for f in group if f] if r]\n",
    "    group_filtered = [g for g, f in zip(group, state_years) if f and g]\n",
    "    if not group_filtered:\n",
    "        print('skipping group starting with file {}'.format(group[0]))\n",
    "        continue\n",
    "    files = sc.parallelize(group_filtered, n_executors)\n",
    "    data = files.flatMap(process_closure)\n",
    "    data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    header = data.sample(False, 0.01).mapPartitions(get_keys)\\\n",
    "        .filter(lambda x: x is not None).distinct().collect()\n",
    "    header = sc.broadcast(header)\n",
    "    data = data.map(lambda x: Row(**{h: x.get(h, None) for h in header.value}))\n",
    "    # #     Infer the schema, and register the DataFrame as a table.\n",
    "    person = sql.createDataFrame(data, samplingRatio=0.4)\n",
    "    person.createOrReplaceTempView(\"person\")\n",
    "    person.write.parquet('/user/hive/warehouse/dataworld/person/', \n",
    "                             mode='append', compression='snappy', partitionBy=['state', 'year'])\n",
    "    data.unpersist()\n",
    "# afew = data.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keys(x):\n",
    "    '''Take a sample from each file and get the questions of the survey.\n",
    "    '''\n",
    "    return list(set(chain(*[list(d.keys()) for d in x])))\n",
    "\n",
    "header = sc.broadcast(header)\n",
    "data = data.map(lambda x: Row(**{h: x.get(h, None) for h in header.value}))\n",
    "# #     Infer the schema, and register the DataFrame as a table.\n",
    "person = sql.createDataFrame(data, samplingRatio=0.4)\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "person.write.parquet('/user/hive/warehouse/dataworld/person/', \n",
    "                         mode='append', compression='snappy', partitionBy=['state', 'year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n'.join([\"ALTER TABLE person ADD IF NOT EXISTS PARTITION (state='{}', year='{}');\".format(state, year) for state, year in state_year_hive]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sql.read.parquet('/user/hive/warehouse/dataworld/person')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spark_utils.clean_dataworld import process_file, process_record\n",
    "from inspect import getsource\n",
    "\n",
    "print(getsource(process_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
